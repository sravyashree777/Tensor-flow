[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Quarto Blog",
    "section": "",
    "text": "Simple Visualisation\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nWelcome Post\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nHello world, this is my first blog post.\nI can write in markdown\nprint(\"Hello World\")\nI can also write math equations:\n\\[\ny = x^2\n\\]\nI can create lists easily:\n\nOne\nTwo\n\nI can also create numbered lists:\n\nOne\nTwo\n\nOr, create a table:\n\n\n\nName\nAge\n\n\n\n\nAlice\n20\n\n\nBob\n21"
  },
  {
    "objectID": "posts/visualisation.html",
    "href": "posts/visualisation.html",
    "title": "Simple Visualisation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tensor flow probability",
    "section": "",
    "text": "A Powerful Library for Probabilistic Modeling and Inference"
  },
  {
    "objectID": "index.html#tensorflow-probability",
    "href": "index.html#tensorflow-probability",
    "title": "Tensor flow probability",
    "section": "",
    "text": "A Powerful Library for Probabilistic Modeling and Inference"
  },
  {
    "objectID": "index.html#install-from-source",
    "href": "index.html#install-from-source",
    "title": "Tensor flow probability",
    "section": "Install from source",
    "text": "Install from source\n  sudo apt-get install bazel git python-pip\n  python -m pip install --upgrade --user tf-nightly\n  git clone https://github.com/tensorflow/probability.git\n  cd probability\n  bazel build --copt=-O3 --copt=-march=native :pip_pkg\n  PKGDIR=$(mktemp -d)\n  ./bazel-bin/pip_pkg $PKGDIR\n  python -m pip install --upgrade --user $PKGDIR/*.whl"
  },
  {
    "objectID": "index.html#probability-distributions",
    "href": "index.html#probability-distributions",
    "title": "Tensor flow probability",
    "section": "1.Probability Distributions",
    "text": "1.Probability Distributions\nTFP provides extensive support for built-in probability distributions, such as:\n\nDiscrete Distributions: Bernoulli, Categorical, Poisson\nContinuous Distributions: Normal (Gaussian), Exponential, Beta, Gamma\nMultivariate Distributions: Multivariate Normal, Dirichlet\n\nUses: These distributions assist in the characterization of random variables, representation of uncertainties, and probabilistic calculations."
  },
  {
    "objectID": "index.html#bayesian-inference",
    "href": "index.html#bayesian-inference",
    "title": "Tensor flow probability",
    "section": "2. Bayesian Inference",
    "text": "2. Bayesian Inference\nBayesian inference updates belief from data based on observations applying Bayesâ€™ Theorem. TFP makes available such tools as Bayesian Neural Networks (BNNs) to use in deep learning the uncertainty.\nUses: As opposed to other neural networks that yield point estimates, Bayesian models in TFP produce probability distributions over outcomes, enhancing decision-making in environments where there is uncertainty."
  },
  {
    "objectID": "index.html#markov-chain-monte-carlo-mcmc",
    "href": "index.html#markov-chain-monte-carlo-mcmc",
    "title": "Tensor flow probability",
    "section": "3. Markov Chain Monte Carlo (MCMC)",
    "text": "3. Markov Chain Monte Carlo (MCMC)\nMCMC is a sampling algorithm for complex probability distributions. TFP offers fast implementations such as:\n\nHamiltonian Monte Carlo (HMC)\nNo-U-Turn Sampler (NUTS)\n\nUses: MCMC enables the estimation of posterior distributions if direct computation is not feasible, and thus it is a tool for Bayesian inference."
  },
  {
    "objectID": "index.html#variational-inference-vi",
    "href": "index.html#variational-inference-vi",
    "title": "Tensor flow probability",
    "section": "4. Variational Inference (VI)",
    "text": "4. Variational Inference (VI)\nVariational inference is an optimization-driven approach for approximating probability distributions. TFP offers mechanisms such as:\n\nReparameterization Tricks for optimal learning\nAutomatic Differentiation Variational Inference (ADVI)\n\nUses: VI is generally quicker than MCMC for big data and is extensively applied in deep learning for Bayesian Neural Networks."
  },
  {
    "objectID": "index.html#gaussian-processes-gps",
    "href": "index.html#gaussian-processes-gps",
    "title": "Tensor flow probability",
    "section": "5. Gaussian Processes (GPs)",
    "text": "5. Gaussian Processes (GPs)\nGaussian Processes are parametric-free models which present uncertainty-informed predictions. With TFP, users can:\n\nDefine custom GP regression models\nUse kernel functions like Radial Basis Function (RBF)\n\nUses: GPs are excellent for time-series prediction, spatial modeling, and reinforcement learning, where uncertainty is important."
  },
  {
    "objectID": "index.html#joint-probability-distributions",
    "href": "index.html#joint-probability-distributions",
    "title": "Tensor flow probability",
    "section": "6. Joint Probability Distributions",
    "text": "6. Joint Probability Distributions\nTFP facilitates the construction of joint distributions with the JointDistribution module, which assists in modeling interdependencies among several variables.\nUses: Critical to probabilistic graphical models, hidden Markov models, and structured probabilistic modeling."
  },
  {
    "objectID": "index.html#tensorflow-integration",
    "href": "index.html#tensorflow-integration",
    "title": "Tensor flow probability",
    "section": "7. TensorFlow Integration",
    "text": "7. TensorFlow Integration\nBecause TFP is developed based on TensorFlow, it takes advantage of:\n\nGPU acceleration for faster computations\nAutomatic differentiation for gradient-based optimization\nSeamless integration with TensorFlow models\n\n**Uses:* This renders TFP scalable, efficient, and simple to use with deep learning models."
  }
]